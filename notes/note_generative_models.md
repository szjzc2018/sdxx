# Generative Models

##  Introduction
生成式模型(Generative Models)的目标是生成具有某种特定特征的图片(e.g.数字，人脸......)

一类常见的生成式模型是概率生成模型，这类模型学习一个概率分布$f(x)$，其“生成结果”就是从$f$中采样得到的结果，因此，我们希望让“好”的图片在$f$中拥有尽可能高的概率，这样我们从$f$中采样，就会大概率得到想要的图片。我们接下来主要讨论这种模型。

## Loss Function

既然要训练一个模型，首先当然要定义一个评判标准，也就是loss function. 对于Discriminative Model, 我们可以很方便地定义“好”与“不好”，只需要看模型的判断准确率就行了（或者技术上，一般用连续的cross-entropy loss来衡量模型的好坏）。

但是，对于生成式模型，是否可以定义一个评判生成好坏的函数$V(x)$,然后来最大化$\mathbb{E}_{x\sim f}V(x)$？然而，答案一般是否定的，生成任务一般没有明确的好坏评判标准，更不用说写成一个可以用于实际计算训练的函数了。于是，我们退而求其次，既然我们有训练数据集，而训练数据集外的不好定义，何不直接只把数据集中的数据定义成“好的”呢？（事实上，或许你会想到另一种思路，是否可以再用一个神经网络来学习这个函数$V(x)$呢？这个思路也引出了之后将会说到的GAN模型）

于是，假如我们的训练集叫做$P$。定义$P$中的元素的$V$值为1，其余全为0的话，相当于我们在最大化$L_0=\mathbb{E}_{x\sim f}V(x)=\sum_{x\in P}p(x)$。初看上去似乎有合理性，但是这个函数有两个问题：首先，由于$x$一般是一个高维向量，所以$p(x)$一般来说比较小，导致$L_0$的值也非常小，可能出现精度以及不稳定等问题；其次，可以很容易发现事实上以1的概率输出数据集中某张特定图片的函数就可以使$L_0$达到最大值1，但是这样的函数显然是没有意义的。

所以，一个好的损失函数至少应该满足三个性质：
- 1.要能体现训练集上的图片的“好”；
- 2.不能是特别小的量(不能和$p$同等量级)；
- 3.不能有trivial的极值使得模型只关注数据集中的少数图片

于是，这就引出了我们的损失函数估计MLE(Maximum Likelihood Estimation):
$$L=\frac{1}{|P|}\sum_{x\in P}\log p(x)$$

我们要对$L$做梯度上升使之最大化（或者如果不习惯梯度上升，也可以对$-L$梯度下降）。可以看到这个函数通过增加一个log函数，在保留性质1的同时完美地解决了性质2和3的问题，此时$L$的理论最大值在$p(x)$在$P$里每处都取$\frac{1}{|P|}$时取到，并且一但某个图片被忽略，概率极小，那么整个$L$会受到很大影响。

到这里，或许你会提出疑问，既然我们的损失函数只把$P$中的图片定义成“好”的而不区分别的图片，那么是否会导致模型只生成$P$中的图片？确实，生成任何$P$以外的图片对增大$L$并没有好处，所以理论上来说最优的策略确实是只生成$P$中的图片，但这其实就相当于discriminative model中的过拟合问题，想想之前我们在discriminative model时，是不是理论最优的模型也是记住所有训练集的照片和标签，然后根据搜索标签直接输出结果？但是由于训练集过大，从理论上来说因为参数空间的大小小于训练空间的大小，所以这种情况不会发生，我们也才有理由相信模型是学到了特征而不是记住了训练集。同时，我们也可以用类似的避免过拟合的方法来解决这个问题，但这都是后话，对于设计训练模型，最好的方法还是"First overfit, then regularize"。

直观地说，我们学习的确实是一个store $P$的函数，但是这个函数的参数量远小于$P$的大小，所以我们的模型是有泛化能力的。

## 1. Energy-based Model

既然我们的模型就是一个概率分布$p(x)$, 那么最简单粗暴的方法当然就是直接对每个$x$用某个函数来表示他的概率，同样为了让相对大小在正常的量级上方便计算(同时也受物理学方面的一些启发，以及早期的一些network结构的研究)，我们考虑去对于每个$x$定义一个能量函数$E(x)$正比于 $-\log p(x)$，这唯一确定了概率分布$p(x)=\frac{1}{Z}e^{-E(x)}$，其中$Z$是归一化因子。

### 1.1 Hopfield Network:Intro

一个典型的例子是Hopfield Network,它的输出是$\{-1,1\}^n$ 中的向量，而能量函数被定义为$E(x)=-\frac{1}{2}x^TWx$，其中$W$是一个对称矩阵。

（值得一提的是，事实上，Hopfield Network最初并不是一个概率生成模型，而是一个用来store pattern的确定的能量模型，注意到如果让$w=xx^T$,那么就可以知道$x$的能量唯一最小，对于一般的多个$x$,我们也希望通过构造适当的$W$让$x$们是能量极小点，从而通过在网络中梯度下降寻找极小值来找到储存的$x$.然而，有论文证明了，$N$个点的Hopfield Network 最多储存 $O(N \log N)$
个pattern, 并且这个上界是否可以改进还并不知道，这也从某种角度上说明了前面我们所担心model 会记住数据集的“过拟合”的问题并没有那么严重。）

回到原来的network(事实上，基于hopfield network的概率生成模型被称为Boltzmann Machine)，既然已经定好了能量函数，接下来的问题就是如何训练和如何从概率分布里生成的问题了。

### 1.2 Hopfield Network:Training

一般的Energy Based Model 的损失函数可以写成
$$ L=\frac{1}{|P|}\sum_{x\in P}\log \frac{e^{-E(x)}}{Z}=\frac{1}{|P|}\sum_{x\in P}(-E(x)-\log Z)$$
从而
$$ \nabla L = \frac{1}{|P|}\sum_{x\in P}-\nabla E(x)-\frac{\nabla Z}{Z}$$
然而，由于$Z$也在不断变化，而且一般根本无法计算，这个梯度及其难求，这也是Energy-based model 最大的缺点之一。（这也是自然的，因为理论上来说Energy-based model 可以对应所有的概率分布， 如果对于一般的energy-based model 有好的优化方法的话就解决了所有的概率生成模型的问题了）

事实上，我们定义的这个概率函数有着非常好的性质，导致梯度可以用期望来表示。简单的计算表明
$$\nabla_W(L)=\frac{1}{|P|}(\sum_{x\in P}x^Tx)-\mathbb{E}_{x \in D}x^Tx$$
其中$D$是当前的概率模型所代表的概率分布。（我们在下文中也采取这个记号表示当前模型所对应的概率分布）

前面一项可以直接计算（在SGD中就是对取出的batch计算），而后面的一项则涉及从$D$中采样的问题，这在最后生成图片的过程中也是需要的。

这里，我们使用***Gibbs Sampling***的方法采样，具体地，先随机取一个$x=(x_1,\dots,x_n)$,然后以概率$(x_i|x_1,\dots,x_{i-1},x_{i+1},\dots,x_n)$更新$x_i$的值，重复直到收敛（in pratice, 可能会出现在某个小范围震荡不收敛的情况，这个时候我们近似认为这个小区域内每个都差不都，于是实际的过程就是重复更新足够多次然后取一个）

从而，我们得到了总流程：
- 1.初始化$W$
- 2.训练：对于每个batch
    - 2.1 计算batch中的$x^Tx$的平均值
    - 2.2 通过若干次Gibbs Sampling 估计$\mathbb{E}_{x \in D}x^Tx$
    - 2.3 计算梯度并更新$W$
- 3.生成：通过Gibbs Sampling 从当前模型中采样

值得一提的是，为了提高模型的表现力，我们可以引入一些Hidden Neurons. 但上面的训练过程即使在没有Hidden Neurons的情况下也效率非常低，主要在于Gibbs Sampling 非常耗费时间，并且稳定性也得不到保证，于是，我们可以对原来的图做一点小改动，我们构造一些Hidden Neuron 和 Visible Neuron 形成一个二分图(也就是说，$W$ 是有两块是$O$的$2\times2$分块矩阵),此时可以发现我们Gibbs Sampling 的每一步可以从更新一个分量变成更新所有Visible Nerons 或 Hidden Neurons，从而大大提高了效率。

### 1.3 Sampling Method for Energy-based Model

在上面的的讨论中，我们看到了一个典型的energy-based model的训练方法。在Hopfield Network 中，由于每个分量都是 1 或 -1，我们可以比较方便地进行Gibbs Sampling， 这是因为固定(n-1)个分量以后，条件概率变成了简单的两点分布，然而，对于一般的连续值的模型，就没有这么好的事情了（我们可能很难算出$p(x_1,x_2,\dots,x_{i-1},x_{i+1},\dots,x_n)$的值）

这里，我们介绍两个Sample的方法：**Importance Sampling** 和 **Metropolis-Hastings Algorithm**

#### 1.3.1 Importance Sampling

Importance Sampling 用来解决计算在某个分布下估计函数期望的问题，具体地，如果我们要估计$\mathbb{E}_{x \sim p}f(x)$, 但从$p$
中采样及其困难，因为我们只有未归一化的概率而没有归一化系数。那么，我们可以找一个简单，好采样的分布$q$, 然后估计$\mathbb{E}_{x \sim q}f(x)\frac{p(x)}{q(x)}$，这两个期望在数学上是相等的，然而通过“换元”我们得到了一个更好采样的分布。

我们可以用一个例子来理解Importance Sampling: 假如面前有$N$箱不同的水果，每箱水果的种类和个数都不一样（单个箱子里的水果重量都是一样的），我们想通过采样来算出单个水果的平均重量。(重量代表$f$,每箱水果代表一个$x$,个数代表未归一化的$p$) 

一个自然的想法是随机取若干个水果取平均，然而，由于箱子数太多，我们根本不能全打开，从而我们不知道水果的总个数，于是无法真正“均匀随机”取出水果！于是自然的想法就是我们均匀随机取1~N对应的箱子，对这些箱子中所有水果的重量取平均，这样我们就得到了一个估计值。这个估计值的期望就是我们想要的平均重量，这对应的是$q$是uniform的情况。$q$非uniform 的情况的intuition是，假如我们知道1~10号箱子里的水果特别多，占了总数的90%，然而在1~N的均匀随机取样中却有可能根本取不到这些箱子，导致估计值严重缺乏真实性。

于是，我们可以将这些箱子$x$每个都拆成$q(x)$个小“箱子”，这样就可以使取到的概率提升，但每箱中水果的个数就变成了$\frac{p(x)}{q(x)}$个，这就对应了我们的公式。

从这个例子里可以看出，因为我们最理想的情况其实是均匀随机取出一个水果采样，所以最优策略是要让每个小箱子里水果数都一样，也就是$p$和$q$尽可能接近。从数学上可以证明，当它们正比时，估计值的方差最小。

#### 1.3.2 Metropolis-Hastings Algorithm

Metropolis-Hastings Algorithm 是Gibbs Sampling 的推广，具体地，我们构造一个马尔可夫链，使得其平稳分布是我们要采样的分布，然后通过模拟这个马尔可夫链的状态转移来采样。

具体地，我们先构造一个proposal distribution $q(x'|x)$, 然后我们从$x$转移到$x'$的概率是
$$A(x\rightarrow x')=\min\{1,\frac{p(x')q(x|x')}{p(x)q(x'|x)}\}$$
这个公式的直观理解是，如果$p(x')q(x|x')$比$p(x)q(x'|x)$大，那么我们就一定接受这个转移，否则我们以$\frac{p(x')q(x|x')}{p(x)q(x'|x)}$的概率接受这个转移。
可以证明，在函数满足一些连续性和有界性条件的情况下，这个马尔可夫链的平稳分布是$p$。

## 2.Normalizing Flow

### 2.1 Intro

## 3. Variational Autoencoder

### 3.1 Intro

VAE的intuition来自于：一般来说，图片都有自己的特征，于是，我们可以显式地定义一个特征空间$Z$，然后再从这个特征空间生成图片。也就是说，我们训练一个概率分布$p(x,z)$,其中$z$是简单的分布，比如高斯分布，然后通过$p(x|z)$生成图片。

### 3.2 Design the structure

我们仍然采取和之前一样的的MSE估计，此时，
$$L=\frac{1}{|P|}\sum_{x\in P}\log(\sum_z p(x,z))$$

简单起见，我们考虑求和中的一项$\log(\sum_z p(x,z))$,
由于这个式子本身就不好计算，梯度更加难算，我们用1.3.1中的Importance Sampling来估计这个值，具体地，我们引入一个分布$q(z)$,然后我们可以估计这个值为
$$\log p(x)=\log \sum_z q(z)\cdot \frac{p(x,z)}{q(z)} \ge^{Jensen}\sum_z  q(z)\log \frac{p(x,z)}{q(z)}$$

我们称右侧的这个下界为ELBO(Evidence Lower Bound)。
从而，我们可以通过从$q$中sample来估计后者从而估计前者。同时，为了让前后尽可能接近，我们训练$q$使得两者误差最小。注意到右侧事实上是
$$ELBO=\sum_z q(z)\log \frac{p(z|x)p(x)}{q(z)}=\log p(x)-KL(q(z)||p(z|x))$$

所以，在主训练步骤中，我们想要让$\log p(x)$尽量小，从而达到更好的效果，在训练$q$的过程中，我们想让它和$p$尽量接近，从而让取样模拟真实情况，方差更小。这时候再看上面那个式子，我们惊奇地发现，事实上这两件事都是在让ELBO最大化！所以，我们的两个过程都可以用一个统一的损失函数ELBO来训练。（你可能会注意到，这里虽然一切都吻合地非常好，但是也都是intuition,没有特别的必然性，所以其中的步骤仍然有调整的空间，$\beta-VAE$就是修改我们的ELBO，在KL散度前面加系数，使之取得更加灵活的效果）

**所以，我们的损失函数可以定义为上面的ELBO!**

至此，我们已经做出了一个比较完整的分析，在进入训练步骤之前，我们还需要考虑一个小小的改动。前面为了偷懒，我们只考虑了一个$x$的情形，然而，我们不能对每个$x$训练一个单独的$q$,于是我们转而考虑训练一个神经网络$q(z|x)$. 于是，我们的总体目标变成了训练两个神经网络$q_{\phi}$和$p_{\theta}$,使得
$$ELBO=\frac{1}{|P|}\sum_{x\in P}\sum_z q(z|x)\log\frac{p(z)}{q(z|x)}$$
$$=\frac{1}{|P|}\sum_{x\in P}\sum_z q(z|x)\log\frac{p(x|z)p(z)}{q(z|x)}$$
$$=\mathbb{E}_{z\sim q(z|x)}[\log p(x|z)]-KL(q(z|x)||p(z))$$

经过上面的数学变形，我们发现，这个loss有了直观的意义：第一项称为reconstruction loss, $q$就像一个编码器，把图片$x$编码成特征$z$，而 $p$ 则想以最大概率恢复原来的图片，第二项称为KL散度，它想让$q$和$p$尽量接近，从而让$q$的取样和真实情况更接近。

总结：
我们通过importance sampling的下界，找到了一个关键的损失函数$ELBO=\sum_z q(z|x)\log \frac{p(z)}{q(z|x)}$,它有两种表示
$$ELBO=\log p(x)-KL(q(z|x)||p(z|x))$$
$$ELBO=\mathbb{E}_{z\sim q(z|x)}[\log p(x|z)]-KL(q(z|x)||p(z))$$

第一种表示说明它在生成模型的训练和proposal的训练中都起到了evaluation的作用，所以可以作为统一的损失函数，而第二种表示则给出了直观的意义，同时变为了方便采样计算的结构。

### 3.3 choose of $q$ and $p$
为了计算方便，我们取
$$p_{\theta}(z)=N(0,I)$$
$$p_{\theta}(x|z)=N(f_{\theta}(z),I)$$
$$q_{\phi}(z|x)=N(\mu_{\phi}(x),diag(\exp(\sigma_{\phi}(x))))$$
其中$f_{\theta},\mu_{\phi},\sigma_{\phi}$都是神经网络。

### 3.4 Training

回到$ELBO$的可采样形式
$$ELBO=\mathbb{E}_{z\sim q_{\phi}(z|x)}[\log p_{\theta}(x|z)]-KL(q_{\phi}(z|x)||p_{\theta}(z))$$

我们注意到第二项就是两个高斯分布的KL散度，可以直接计算，而第一项则是一个期望，我们可以通过采样来估计。
具体地，当计算第一项对$\theta$的梯度的时候，可以从$q_{\phi}(z|x)$中采样若干个$z$，计算梯度平均值。而计算对$\phi$的梯度的时候，由于“采样”无法求导，我们可以通过一个小技巧，即把$N(a,b)$看成$a+b\cdot N(0,1)$, 这样我们的采样只涉及从标准正态分布中采样，从而总函数可以对$\phi$求导。一般来说，由于数据的随机性已经足够强，我们只进行一次采样估计。

综上所述，总训练过程如下：
- 1.初始化
- 2.训练
    - 2.1 训练$\theta$
        - 2.1.1 从$q_{\phi}(z|x)$中采样$z$
        - 2.1.2 通过采样估计$\nabla_{\theta}ELBO$
        - 2.1.3 更新$\theta$
    - 2.2 训练$\phi$
        - 2.2.1 从N(0,I)中采样$\epsilon$,根据它生成$z$
        - 2.2.2 估计$\nabla_{\phi}ELBO$
        - 2.2.3 更新$\phi$
- 3.生成
    - 3.1 从$p$中采样$z$
    - 3.2 从$p_{\theta}(x|z)$中采样$x$

### 3.5 Others

#### 3.5.1 impainting

为了图片补全，我们需要$x\to z$的过程尽量robust, 这样我们就可以用需要补全的图片生成$z$，然后用$p_{\theta}(x|z)$生成图片。此时，我们可以在训练过程中随机mask掉一些neurons，从而让模型学会robust的特征。

#### 3.5.2 $\beta$-VAE

就是在上面已经说明的，通过在KL散度前面加一个系数$\beta$，从而可以调整reconstruction loss和KL散度的权重，从而可以调整模型的表现。

#### 3.5.3 Conditioned VAE

当数据有标签的时候应该如何处理？
此时，$p$变为$p_{\theta}(x|y,z)$,
$q$变为$q_{\phi}(y,z|x)$

对有标签的数据，我们只需要在loss中加入一项cross-entropy loss between $q(y|x)$和 $y$即可

对无标签的数据，我们让KL penalty 变成$KL(q(z)||p(z))+KL(q(y)||p(y))$, 对于$p(y)$我们可以取一个uniform distribution. 而 reconstruction loss 则变成$\mathbb{E}_{z,y\sim q(z,y)}[\log p(x|z,y)]$ 但此时$y$会变得难以sample从而不好计算梯度，当种类比较少的时候可以通过每一类枚举来计算，而种类比较多的时候我们将要使用一些特别的办法来处理，将会在之后说明。


## 4. Generative Adversarial Network

### 4.1 Intro