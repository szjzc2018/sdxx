# Deep Learning Lecture Notes

## 0. Introduction

### 0.1 General Introduction Of Deep Learning
> Deep Learning is a class of machine learning methods that use neural networks to learn representations from raw data.

什么是 deep learning? 简单的说， deep learning 是利用深度神经网络学习数据的表示的一种机器学习方法。这里的“深度”指的是神经网络的层数，而“学习数据的表示”指的是神经网络的参数，也就是说，我们通过调整神经网络的参数来学习数据的表示。
> There're two kinds of model, one is the discriminative
> model from $X$ to $Y$, the other is the generative model
> from $Y$ to $X$.(Yi Wu)

更加数学化地，我们有一个目标函数$f$,而我们希望用一个带参数的神经网络$NN_\theta$去拟合$f$，使得$NN_\theta(x) \approx f(x)$,这就是深度学习想做的事情，而找到这个$\theta$的过程就是 ~~炼丹~~ 训练(train)。

根据所学习的$f$的不同，我们可以把任务分成两类，从而对应两类不同的模型：生成式模型(Generative Models)和判别式模型(Discriminative Models)。例如，当数据是手写数字的图片时，生成式模型的目标是给定一个数字，生成一张图片，而判别式模型的目标是给定一张图片，判断这张图片是什么数字。

这两者在数学上的本质区别实际上是$f$的性质，判别式模型学习的$f$ 是一个**映射**$f(x)=y$,也就是说给定一个$x$(一般称作data),我们就有唯一对应目标的$y$(一般称作label,或许你会说对于噪声图片$x$似乎也没有我们想要对应的$y$，但是这里的讨论中，把定义域先当成我们想要的图片(训练图片，测试图片...))，而生成式模型则是给定$y$,希望输出一个$x$,学习的是**映射的"反函数"**$f^{-1}(y)=x$.(也许追求严谨的人到这里已经看不下去了，但别急，后面有详细的解释)

到这里，你可能会发现我们一直在回避一个定义上的问题，这个函数的定义域和值域究竟是什么？还是回到手写数字-标签的例子，对$y$的值域$Y$这是简单的，就是所有标签的集合，而对于$x$的定义域，事实上，作为一个神经网络，它可以接受的$x$自然是所有可能图片的集合，然而我们并不关心它在大多数噪声图片上的表现，而是只关心我们想要的真正的数字图片的识别效果，也就是说，加入我们把所有图片的集合设为$\Omega$,那我们关心的只是$\Omega$的一个小子集$X_0$,代表所有手写数字的图片的集合。

对于判别式模型, 我们希望$NN(x)\approx f(x)(x\in X_0$).

$X_0$究竟是什么呢?我们可以发现这和我们任务的定义有关，并没有数学上严格的说法。我们有什么？一般来说，我们有一个由一些满足$f(x)=y$的$(x,y)$构成的训练集$P$, 并且可以确定的是，$P$中的$x$一定被$X_0$包含，而我们凭借训练数据也知道不了更多了！所以，事实上，神经网络的训练只是关心$NN(x) \approx f(x) (x\in P)$, 而我们默认了$X_0$ 有一些比较好的内在的性质，使得这可以让$NN(x) \approx f(x) (x\in X_0)$也成立。这称为泛化能力(Generalization Ability)。 

这个假设在数学上一般当然不成立，我们称满足$NN(x) \approx f(x) (x\in P)$ 但是不满足$NN(x) \approx f(x) (x\in X_0)$的现象为过拟合(Overfitting),而研究如何避免过拟合是判别式模型训练的重要内容,对于模型泛化能力的研究也是
深度学习理论研究的重点防线之一。你或许已经可以在上面我们问题的表述中想到一点如何避免过拟合的想法，我们将在下面的章节中详细讨论。

> “或许你overfit的模型，学到的正是某个平行世界中的特征”(Yao Seminar)

对于生成式模型，定义就更加模糊了。Generally speaking, 我们希望找到$NN$,使得给定$y$,我们可以生成一个$NN(y) = x$, 使得 $x \in X_0$, 并且 $f(x) = y$. 

此时，我们的$P$可以和之前一样，是满足$f(x) = y$并且 $x \in X_0$的$(x,y)$, 但我们注意到这个时候没有标签的$x \in X_0$也有了意义，因为我们可以通过它学到$X_0$的信息！所以，根据训练数据是否有标签，我们可以把生成式模型的训练分成有监督(supervised learning),无监督(unsupervised learning)和半监督(semi-supervised learning)三种情况,分别对应数据全部有标签，全部无标签和部分有标签的情况。(显然，对于无监督情况，我们不能完成给定标签的生成)

当然，上面关于目标的定义显然可以找到很多cheat的方法，例如直接从训练集中找一张符合要求图片输出就可以达到目标，所以这里就涉及到评判模型好坏的标准了。

我们先说比较简单的评判判别式模型好坏的方法，一般来说是找$X_0$的另一个子集$P'$,称为测试集,然后用$NN(x)$和$f(x)$在$P'$上的差距来衡量，
而有一些大家都认可的测试集，从而可以比较大家生成模型的好坏。

然而，对于生成式模型，就没那么简单了，首先$X_0$没有明确数学定义，而
在判别式模型里我们只需要一个公认的$X_0$的子集就行了，但在生成式模型里，我们似乎一定得搞明白$X_0$究竟是什么！于是，这导致生成式模型到现在也没有一个universal的标准，而对于不同的生成式模型，我们也会有不同的评判标准，这在后面会详细讨论。但回到我们关于$X_0$的定义，不就是“手写数字”的集合的主观定义嘛！所以有一个“耍赖”的评判标准——直接主观判断！这也是为什么许多生成式模型的论文里，往往会放很多生成的图片的原因，这也确实在某种方面上更加有说服力。

### 0.2 Training

深度学习发展至今，对于给定的网络结构，如何训练网络已经有了很多成熟的方法，其中的核心就是优化理论中的重要算法:梯度下降。

于是，模型的训练现在以及变成了一个程序化的过程:
- 1.定义模型结构
- 2.定义一个损失函数，表示模型的好坏
- 3.定义一个基于梯度下降的优化算法，使得损失函数最小化
- 4.根据算法优化损失函数，得到最终模型

所以，对于任何一个任务，我们就分成了四个步骤，而这四个步骤中的每一步都非常重要，直接关系到最终生成的结果。我们之后对于模型的介绍也基本会按照这个框架来进行。对于第三步，由于它和模型无关，所以我们会在下面先做讨论，对之后的所有模型，方法都是类似的。

### 0.3 Gradient Descent

#### 0.3.1 GD

当然，最简单的办法就是直接用梯度下降，也就是说，我们直接计算损失函数对于参数的梯度，然后按照梯度的方向更新参数。这个方法的优点是简单直接，但对于大多数情况来说，根本不现实，大部分训练集的规模都是非常大的，而计算梯度的时间复杂度也不低。

#### 0.3.2 SGD

于是，我们引入了SGD(Stochastic Gradient Descent),也就是说，我们不是计算整个训练集的梯度，而是每次只取数据集的一部分，称为一个batch，计算这个batch的梯度，然后更新参数。这样做可以显著减少计算量，这也是深度学习中常见的优化算法。(事实上，人们后来发现，SGD因为内在的随机性，还可以有效减轻过拟合问题)

实际应用中，为方便起见，一般都是直接把整个测试集分成若干个batch，然后每次取一个batch进行训练，对所有batch训练完一遍称为一个epoch，然后重复若干个epoch直到收敛。**之后的所有优化算法都是基于SGD的改进，核心思想都是通过减小震荡，加快收敛,具体的数学表达式可能比较复杂或者奇怪，而数学上的严格证明也大多和实际应用有一定的差距，所以我们在这里只做简单的介绍。~~不想看可以直接跳到下一节~~**

SGD的数学表达式可以写成这样:$$\theta_{t+1}=\theta_t-\eta \nabla \mathbb{E}_{x \sim B}[L(\theta_t, x)]$$,
其中$B$是一个batch, $\eta$是学习率，$L$是损失函数。

#### 0.3.3 Momentum

然而，SGD有一个问题，就是在到达最优点的时候，由于梯度的方向不断变化，可能会导致震荡，于是，我们引入了Momentum,也就是说，我们不仅仅考虑当前的梯度，还考虑之前的梯度，这样可以减小震荡，加快收敛。

Momentum的数学表达式可以写成这样:$$v_{t+1}=\gamma v_t+\eta \nabla \mathbb{E}_{x \sim B}[L(\theta_t, x)]$$
$$\theta_{t+1}=\theta_t-v_{t+1}$$
其中$\gamma$是一个超参数，一般取0.9左右。(超参数指的是不通过训练得到的参数，而是需要人为设定的参数，区别于神经网络的参数)

#### 0.3.4 Adam(Adaptive Moment Estimation)

Adam是一种结合了SGD和Momentum的优化算法，它的优点是可以自适应地调整学习率，从而可以更好地适应不同的数据集。

Adam的数学表达式可以写成这样:$$m_{t+1}=\beta_1 m_t+(1-\beta_1)\nabla \mathbb{E}_{x \sim B}[L(\theta_t, x)]$$
$$v_{t+1}=\beta_2 v_t+(1-\beta_2)(\nabla \mathbb{E}_{x \sim B}[L(\theta_t, x)])^2$$
$$\hat{m}_{t+1}=\frac{m_{t+1}}{1-\beta_1^{t+1}}$$
$$\hat{v}_{t+1}=\frac{v_{t+1}}{1-\beta_2^{t+1}}$$
$$\theta_{t+1}=\theta_t-\eta \frac{\hat{m}_{t+1}}{\sqrt{{\hat{v}_{t+1}}+\epsilon}}$$
其中$\beta_1,\beta_2$是超参数，$\epsilon$是一个很小的数，一般取$10^{-8}$左右。

## 1. Discriminative Model

### 1.1 Structure & Loss Function

回顾我们的目标,我们希望找到$NN_{\theta}$,使得$NN_{\theta}(x) \approx f(x) (x\in X_0)$, 而我们有一些训练数据$P:\{(x,y)\}$,满足 $y=f(x)$. 于是，我们先训练$NN_{\theta}$,使得$NN_{\theta}(x) \approx f(x) (x\in P)$. 然而，对于离散的label,我们很难定义一个比较好的可以求导的损失函数，于是我们引入一个重要的思想:让我们的$NN_{\theta}$对每个标签生成一个概率$p_{\theta,y}(x)$，而我们的目的则变为找到$\theta$使得$p_{\theta,f(x)}(x)$尽可能接近$1$.

于是，这就引出了我们的损失函数的定义:交叉熵(cross-entropy)损失函数:
$$L(\theta)=-\frac{1}{|P|}\sum_{(x,y)\in P}\log p_{\theta,y}(x)$$

(对于一般的两个概率分布$p,q$,交叉熵定义为$H(p,q)=-\sum_x p(x)\log q(x)$,而上面的损失函数就是$p$与一个$f(x)$处单点分布的交叉熵)

### 1.2 Layers

说了这么多，$NN$的结构究竟是什么样？事实上，$NN$的结构是非常灵活（你也可以发明你想要的结构！），这里我们介绍一些常见的结构和它们的特点

#### 1.2.1 Fully Connected Layer & Activation Function

最常见的结构就是全连接层，它的数学表达式可以写成:
$$f: \mathbb{R}^n \rightarrow \mathbb{R}^m, f(x)=\sigma(Wx+b)$$

其中$W \in \mathbb{R}^{m \times n}, b \in \mathbb{R}^m$. 全连接层十分灵活，并且具有很强的表达能力（拟合各种函数的能力），同时可以很方便地进行维度的转换，许多简单的判别任务仅通过全连接层（和马上要说的）激活函数就可以达到比较令人满意的效果。

激活函数$\sigma$是全连接层的重要组成部分，很容易证明，如果没有激活函数的话，事实上多少个全连接层复合都和一个没有区别，并且最终得到的是一个线性的函数，这大多数时候是不够的。(大多数判别任务并没有线性的特征！例如数字识别，把两个0的图片加起来取平均，得到的标签可能是8)常见的激活函数有$sigmoid(x) = \frac{1}{1+e^{-x}}, tanh(x)= \frac{e^x-e^{-x}}{e^x+e^{-x}}, ReLU(x)=\max(0,x)$等等，在不同的任务中，不同的激活函数可能会有不同的效果。

#### 1.2.2 Convolutional Layer
全连接层表达能力非常强，但是，它也存在两个缺点：首先，由于向量的维度一般很大，所以参数量也很大，导致训练时间长；其次，全连接层没有用到数据的任何特点，而是只看成一个向量，这可能会导致过拟合的问题，并且也会浪费我们所知道的关于$f$的某些信息
所以，对于图片类型的数据（这也是深度学习主要应用之一），我们想要引入一个新的结构，这就是卷积层。

卷积层的intuition来自两个观察:首先，对图片，$f$具有一个很好的性质,那就是平移不变性（想象把一张数字2平移1个像素，它仍然是2,并且人眼几乎看不出区别，但如果转化成向量以后，这两个向量也许完全不同）；其次，来自于人判断图片的方式：根据局部的特征来判断图片的内容。于是，我们引入卷积层来获得图片的局部特征。

卷积层的数学表达式可以写成:
$$f: \mathbb{R}^{C_1\times n\times m} \to \mathbb{R}^{C_2\times n'\times m'} $$ 
$$f(x)_{c_2,i,j} = \sum_{k=1}^{k_1}\sum_{l=1}^{k_2}W_{c_1,c_2,k,l}x_{c_1,i+k,j+l}+b_{c_1,c_2,k,l}$$

这看上去很恐怖，但实际上表达的意思就是，我们用$k_1\times k_2$的方框来“扫描”这个图片的每个区域，然后通过一个线性函数（称为卷积核）得到这个区域所对应的值，从而得到新的“图片”，称为特征图。

为什么卷积可以提取局部特征？我们可以用一个简单的例子来直观感受一下，假如我们想确定一张图片（每个像素范围在[0,1]内）中有没有一条长为3的竖线，形如（$\begin{bmatrix}0&1&0\\0&1&0\\0&1&0\end{bmatrix}$），我们可以用一个$3\times 3$的卷积核$\begin{bmatrix}-1&1&-1\\-1&1&-1\\-1&1&-1\end{bmatrix}$来扫描这个图片，如果这个图片中有一条竖线，那么最终得到的特征图中，这个位置所对应的值就应该是3,否则不可能达到3.这就是卷积层提取局部特征的原理，特征图中的每个值都对应着原来图片中的一个局部的特征。

到这里，我们也可以很自然的引入池化层的概念：比如在上面判别竖线的例子中，只有很大的值代表我们提取到的特征，而对于不大不小的值，事实上没有特别大的意义，所以我们可以用最大池化的方式，保留特征图的每个$K\times K$中的最大值。（一个夸张的例子就是在上面的例子中，如果对输出的整个特征图做最大池化，那么最终的输出就是一个$1\times 1$的特征图，如果是3，那么就是有竖线，否则没有）

卷积层（池化层）的优点是参数量小（想象在上面寻找竖线的例子里，如果要用全连接层，我们至少要图片维度大小量级的参数！但是卷积层则只需要9个），可以很好地利用图片的局部特征，而且也可以很好地处理平移不变性的问题。对于卷积层输出的每个值，在原来的图片上和这个值相关的区域成为感受野(receptive field).

PS: 在卷积的“扫描”过程中，我们每次移动的步长也可以不为1,这样就可以起到降维的作用，步长被称为stride.一般地说，如果原图大小为$n\times m$,卷积核大小为$k_1\times k_2$,stride为$s_1\times s_2$,那么输出的特征图大小为$\lfloor \frac{n-k_1}{s_1} \rfloor \times \lfloor \frac{m-k_2}{s_2} \rfloor$. 为了调整输出的大小，我们可以在原图的周围加一圈0，这被称为padding. 如果padding的大小为$p_1\times p_2$（$p_1,p_2$分别是上下,左右的padding行/列数）
,那么输出的特征图大小为$\lfloor \frac{n-k_1+2p_1}{s_1} \rfloor \times \lfloor \frac{m-k_2+2p_2}{s_2} \rfloor$.
### 1.2.3 Batchnorm Layer

Batchnorm是一种用来稳定训练的层，它和pooling层一样没有可训练参数，作用就是把每一层的输出都归一化到均值为0，方差为1的分布，这样可以使得每一层的输出都在一个比较稳定的范围内，从而避免数值过大或者过小导致的梯度消失或者爆炸问题。

### 1.3 Prevent Overfitting

#### 1.3.1 Regularization & Data Augmentation
还记得我们在前面提到的过拟合问题吗?

一般地说，我们只能训练$NN$,使得 $NN(x) \approx f(x) (x\in P)$, 而我们希望$NN(x) \approx f(x) (x\in X_0)$. 这里当然要假定$X_0$有一些比较好的性质，否则这两者之间可能毫不相关。由于$X_0$本身就没有数学上的严格定义，它表示我们实际需要判别的图片，那么，我们可以来思考一下，$X_0$和$P$之间可能有什么关系?

一个很自然的想法就是$X_0$和$P$应该比较接近，从而，如果我们的$f$在比较接近的数据上表现不会差很多的话，效果应该比较好。然而，这一点并不好刻画，我们转而限制一个与之正相关的量：模型的参数大小，这多少蕴含了模型梯度的大小，于是可以起到一定的作用。具体地，我们在损失函数中加入一个正则项，这个正则项一般是参数的范数，这样可以使得参数不会过大，从而避免过拟合。根据加入正则项的不同，我们可以分为L1正则和L2正则，分别是参数的绝对值和平方的和。而这个正则项的系数是一个人为设定的超参数，称为正则化系数(weight decay)。

除了对$f$加以限制以外，我们还有另一个自然的想法：根据实际的情况自己扩展$P$!因为$X_0$本来就是基于某些主观特征和性质定义的东西，所以我们可以直接根据这些性质来生成一些数据拓展$P$,从而让模型在$P$和$X_0$的表现上更一致。例如，在识别数字中，我们知道一个数字随机加一点点小的扰动还是这个数字，于是我们可以把训练的数据加上一点噪声，标签不变，成为新的训练数据。这个过程被称为数据增强(Data Augmentation),常见的数据增强方法有旋转，翻转，缩放等等。
#### 1.3.2 Dropout

Dropout也是一种用来防止过拟合的方法，它主要是为了避免一些偶然出现但不具备generalization能力的非常强的特征。它的思想很简单：在训练的时候，我们随机地让一些神经元失活，也就是说，让它们的输出为0，这样可以使得模型不会过分依赖某些神经元，从而避免过拟合。

#### 1.3.3 Early Stopping

还有一个耍赖的方法:早停(Early Stopping)。它的思想也很简单，既然在$P$ 上的表现和在$X_0$上表现的关系我们弄不清，而事实上我们最终是用测试集来表示$X_0$得到表现，那我们直接把训练集划出一部分作为验证集，然后在验证集上表现不再提升的时候停止训练。这在最终测试集和训练集的分布比较一致的时候是有效的，但是如果训练集和测试集的分布不一致，那么早停可能会导致模型在测试集上表现不好。

### 1.4 Structure

#### 1.4.1 CNN

CNN 的结构是卷积层和池化层的交替，最后加上全连接层，这样的结构可以很好地提取图片的局部特征，而且也可以很好地处理平移不变性的问题。CNN的结构是深度学习中最常见的结构，也是最有效的结构之一。

#### 1.4.2 ResNet

在CNN中，我们可以很容易地发现一个问题：随着网络的加深，梯度消失和爆炸问题会变得越来越严重，从而导致训练困难。ResNet的提出就是为了解决这个问题，它的核心思想是引入了一个shortcut，也就是说，我们不是直接把输入传到输出，而是把输入和输出相加，这样可以使得梯度更容易传播，从而可以训练更深的网络。具体地说，一个ResBlock的结构是这样的:
$$f(x)=x+C(x)$$
其中$C(x)$是一个卷积层（一般由两个卷积层组成），这样，我们就可以很容易地训练一个很深的网络，而且也可以很好地处理梯度消失和爆炸问题。

#### 1.4.3 DenseNet

DenseNet是另一种解决梯度消失和爆炸问题的方法，它的核心思想是引入了一个dense block，也就是说，我们不是把输入和输出相加，而是把输入和输出拼接在一起，这样可以使得梯度更容易传播，从而可以训练更深的网络。具体地说，一个DenseBlock的结构是这样的:
$$ x_0 = input $$
$$ x_i = concat(x_{i-1},C_{i-1}(x_{i-1}))$$

其中$C$是卷积层，$concat$是拼接操作，在每一次操作后，可以发现特征图的个数都在增加，于是到若干层以后，我们使用一个transition layer来减少特征图的个数，这就构成了DenseNet的结构。

## 2. Generative Models

### 2.0 Introduction

还记得我们在Introduction里下的并不严谨的定义吗？

>我们希望找到$NN$,使得给定$y$,我们可以生成一个$NN(y) = x$, 使得 $x \in X_0$, 并且 $f(x) = y$. 

首先的一个问题就是，$y$很少但$x$很多，而我们希望的肯定不是对于每个$y$都输出固定的图片,所以我们想要的$NN$并不是一个数学上的映射！那它究竟是什么呢？由于我们想对$y$输出不同的$x$，于是我们容易想到两种方法：一种是$NN$本身就是一个随机函数，对于固定的$y$,有可能输出很多种不同的$x$; 另一种则是给$NN$ 增加一个随机参数$z$, 使得 $NN(y,z)$是一个映射，并且希望$f(NN(y,z)) = y$. 对于前者，我们介绍Energy-based Model,对于后者，我们介绍VAE，flow model和GAN.

### 2.1 Loss Function: MLE Training

既然要训练一个模型，首先当然要定义一个评判标准，也就是loss function. 对于Discriminative Model, 我们可以很方便地定义“好”与“不好”，只需要看模型的判断准确率就行了（或者技术上，一般用连续的cross-entropy loss来衡量模型的好坏）。

但是，对于生成式模型，是否可以定义一个评判生成好坏的函数$V(x)$,然后来最大化$\mathbb{E}_{x\sim f}V(x)$？然而，答案一般是否定的，生成任务一般没有明确的好坏评判标准，更不用说写成一个可以用于实际计算训练的函数了。于是，我们退而求其次，既然我们有训练数据集，而训练数据集外的不好定义，何不直接只把数据集中的数据定义成“好的”呢？（事实上，或许你会想到另一种思路，是否可以再用一个神经网络来学习这个函数$V(x)$呢？这个思路也引出了之后将会说到的GAN模型）

于是，假如我们的训练集叫做$P$。定义$P$中的元素的$V$值为1，其余全为0的话，相当于我们在最大化$L_0=\mathbb{E}_{x\sim f}V(x)=\sum_{x\in P}p(x)$。初看上去似乎有合理性，但是这个函数有两个问题：首先，由于$x$一般是一个高维向量，所以$p(x)$一般来说比较小，导致$L_0$的值也非常小，可能出现精度以及不稳定等问题；其次，可以很容易发现事实上以1的概率输出数据集中某张特定图片的函数就可以使$L_0$达到最大值1，但是这样的函数显然是没有意义的。

所以，一个好的损失函数至少应该满足三个性质：
- 1.要能体现训练集上的图片的“好”；
- 2.不能是特别小的量(不能和$p$同等量级)；
- 3.不能有trivial的极值使得模型只关注数据集中的少数图片

于是，这就引出了我们的损失函数估计MLE(Maximum Likelihood Estimation):
$$L=\frac{1}{|P|}\sum_{x\in P}\log p(x)$$

我们要对$L$做梯度上升使之最大化（或者如果不习惯梯度上升，也可以对$-L$梯度下降）。可以看到这个函数通过增加一个log函数，在保留性质1的同时完美地解决了性质2和3的问题，此时$L$的理论最大值在$p(x)$在$P$里每处都取$\frac{1}{|P|}$时取到，并且一但某个图片被忽略，概率极小，那么整个$L$会受到很大影响。

到这里，或许你会提出疑问，既然我们的损失函数只把$P$中的图片定义成“好”的而不区分别的图片，那么是否会导致模型只生成$P$中的图片？确实，生成任何$P$以外的图片对增大$L$并没有好处，所以理论上来说最优的策略确实是只生成$P$中的图片，但这其实就相当于discriminative model中的过拟合问题，想想之前我们在discriminative model时，是不是理论最优的模型也是记住所有训练集的照片和标签，然后根据搜索标签直接输出结果？但是由于训练集过大，从理论上来说因为参数空间的大小小于训练空间的大小，所以这种情况不会发生，我们也才有理由相信模型是学到了特征而不是记住了训练集。同时，我们也可以用类似的避免过拟合的方法来解决这个问题，但这都是后话，对于设计训练模型，最好的方法还是"First overfit, then regularize"。

直观地说，我们学习的确实是一个store $P$的函数，但是这个函数的参数量远小于$P$的大小，所以我们的模型是有泛化能力的。

#### 2.1.1. Energy-based Model

既然我们的模型就是一个概率分布$p(x)$, 那么最简单粗暴的方法当然就是直接对每个$x$用某个函数来表示他的概率，同样为了让相对大小在正常的量级上方便计算(同时也受物理学方面的一些启发，以及早期的一些network结构的研究)，我们考虑去对于每个$x$定义一个能量函数$E(x)$正比于 $-\log p(x)$，这唯一确定了概率分布$p(x)=\frac{1}{Z}e^{-E(x)}$，其中$Z$是归一化因子。

##### 1.1 Hopfield Network:Intro

一个典型的例子是Hopfield Network,它的输出是$\{-1,1\}^n$ 中的向量，而能量函数被定义为$E(x)=-\frac{1}{2}x^TWx$，其中$W$是一个对称矩阵。

（值得一提的是，事实上，Hopfield Network最初并不是一个概率生成模型，而是一个用来store pattern的确定的能量模型，注意到如果让$w=xx^T$,那么就可以知道$x$的能量唯一最小，对于一般的多个$x$,我们也希望通过构造适当的$W$让$x$们是能量极小点，从而通过在网络中梯度下降寻找极小值来找到储存的$x$.然而，有论文证明了，$N$个点的Hopfield Network 最多储存 $O(N \log N)$
个pattern, 并且这个上界是否可以改进还并不知道，这也从某种角度上说明了前面我们所担心model 会记住数据集的“过拟合”的问题并没有那么严重。）

回到原来的network(事实上，基于hopfield network的概率生成模型被称为Boltzmann Machine)，既然已经定好了能量函数，接下来的问题就是如何训练和如何从概率分布里生成的问题了。

##### 1.2 Hopfield Network:Training

一般的Energy Based Model 的损失函数可以写成
$$ L=\frac{1}{|P|}\sum_{x\in P}\log \frac{e^{-E(x)}}{Z}=\frac{1}{|P|}\sum_{x\in P}(-E(x)-\log Z)$$
从而
$$ \nabla L = \frac{1}{|P|}\sum_{x\in P}-\nabla E(x)-\frac{\nabla Z}{Z}$$
然而，由于$Z$也在不断变化，而且一般根本无法计算，这个梯度及其难求，这也是Energy-based model 最大的缺点之一。（这也是自然的，因为理论上来说Energy-based model 可以对应所有的概率分布， 如果对于一般的energy-based model 有好的优化方法的话就解决了所有的概率生成模型的问题了）

事实上，我们定义的这个概率函数有着非常好的性质，导致梯度可以用期望来表示。简单的计算表明
$$\nabla_W(L)=\frac{1}{|P|}(\sum_{x\in P}x^Tx)-\mathbb{E}_{x \in D}x^Tx$$
其中$D$是当前的概率模型所代表的概率分布。（我们在下文中也采取这个记号表示当前模型所对应的概率分布）

前面一项可以直接计算（在SGD中就是对取出的batch计算），而后面的一项则涉及从$D$中采样的问题，这在最后生成图片的过程中也是需要的。

这里，我们使用***Gibbs Sampling***的方法采样，具体地，先随机取一个$x=(x_1,\dots,x_n)$,然后以概率$(x_i|x_1,\dots,x_{i-1},x_{i+1},\dots,x_n)$更新$x_i$的值，重复直到收敛（in pratice, 可能会出现在某个小范围震荡不收敛的情况，这个时候我们近似认为这个小区域内每个都差不都，于是实际的过程就是重复更新足够多次然后取一个）

从而，我们得到了总流程：
- 1.初始化$W$
- 2.训练：对于每个batch
    - 2.1 计算batch中的$x^Tx$的平均值
    - 2.2 通过若干次Gibbs Sampling 估计$\mathbb{E}_{x \in D}x^Tx$
    - 2.3 计算梯度并更新$W$
- 3.生成：通过Gibbs Sampling 从当前模型中采样

值得一提的是，为了提高模型的表现力，我们可以引入一些Hidden Neurons. 但上面的训练过程即使在没有Hidden Neurons的情况下也效率非常低，主要在于Gibbs Sampling 非常耗费时间，并且稳定性也得不到保证，于是，我们可以对原来的图做一点小改动，我们构造一些Hidden Neuron 和 Visible Neuron 形成一个二分图(也就是说，$W$ 是有两块是$O$的$2\times2$分块矩阵),此时可以发现我们Gibbs Sampling 的每一步可以从更新一个分量变成更新所有Visible Nerons 或 Hidden Neurons，从而大大提高了效率。

##### 1.3 Sampling Method for Energy-based Model

在上面的的讨论中，我们看到了一个典型的energy-based model的训练方法。在Hopfield Network 中，由于每个分量都是 1 或 -1，我们可以比较方便地进行Gibbs Sampling， 这是因为固定(n-1)个分量以后，条件概率变成了简单的两点分布，然而，对于一般的连续值的模型，就没有这么好的事情了（我们可能很难算出$p(x_1,x_2,\dots,x_{i-1},x_{i+1},\dots,x_n)$的值）

这里，我们介绍两个Sample的方法：**Importance Sampling** 和 **Metropolis-Hastings Algorithm**

###### 1.3.1 Importance Sampling

Importance Sampling 用来解决计算在某个分布下估计函数期望的问题，具体地，如果我们要估计$\mathbb{E}_{x \sim p}f(x)$, 但从$p$
中采样及其困难，因为我们只有未归一化的概率而没有归一化系数。那么，我们可以找一个简单，好采样的分布$q$, 然后估计$\mathbb{E}_{x \sim q}f(x)\frac{p(x)}{q(x)}$，这两个期望在数学上是相等的，然而通过“换元”我们得到了一个更好采样的分布。

我们可以用一个例子来理解Importance Sampling: 假如面前有$N$箱不同的水果，每箱水果的种类和个数都不一样（单个箱子里的水果重量都是一样的），我们想通过采样来算出单个水果的平均重量。(重量代表$f$,每箱水果代表一个$x$,个数代表未归一化的$p$) 

一个自然的想法是随机取若干个水果取平均，然而，由于箱子数太多，我们根本不能全打开，从而我们不知道水果的总个数，于是无法真正“均匀随机”取出水果！于是自然的想法就是我们均匀随机取1~N对应的箱子，对这些箱子中所有水果的重量取平均，这样我们就得到了一个估计值。这个估计值的期望就是我们想要的平均重量，这对应的是$q$是uniform的情况。$q$非uniform 的情况的intuition是，假如我们知道1~10号箱子里的水果特别多，占了总数的90%，然而在1~N的均匀随机取样中却有可能根本取不到这些箱子，导致估计值严重缺乏真实性。

于是，我们可以将这些箱子$x$每个都拆成$q(x)$个小“箱子”，这样就可以使取到的概率提升，但每箱中水果的个数就变成了$\frac{p(x)}{q(x)}$个，这就对应了我们的公式。

从这个例子里可以看出，因为我们最理想的情况其实是均匀随机取出一个水果采样，所以最优策略是要让每个小箱子里水果数都一样，也就是$p$和$q$尽可能接近。从数学上可以证明，当它们正比时，估计值的方差最小。

###### 1.3.2 Metropolis-Hastings Algorithm

Metropolis-Hastings Algorithm 是Gibbs Sampling 的推广，具体地，我们构造一个马尔可夫链，使得其平稳分布是我们要采样的分布，然后通过模拟这个马尔可夫链的状态转移来采样。

具体地，我们先构造一个proposal distribution $q(x'|x)$, 然后我们从$x$转移到$x'$的概率是
$$A(x\rightarrow x')=\min\{1,\frac{p(x')q(x|x')}{p(x)q(x'|x)}\}$$
这个公式的直观理解是，如果$p(x')q(x|x')$比$p(x)q(x'|x)$大，那么我们就一定接受这个转移，否则我们以$\frac{p(x')q(x|x')}{p(x)q(x'|x)}$的概率接受这个转移。
可以证明，在函数满足一些连续性和有界性条件的情况下，这个马尔可夫链的平稳分布是$p$。

#### 2.1.2 Normalizing Flow

#### 2.1.3 Variational Autoencoder

##### 2.1.3.1 Intro

VAE的intuition来自于：一般来说，图片都有自己的特征，于是，我们可以显式地定义一个特征空间$Z$，然后再从这个特征空间生成图片。也就是说，我们训练一个概率分布$p(x,z)$,其中$z$是简单的分布，比如高斯分布，然后通过$p(x|z)$生成图片。

##### 2.1.3.2 Design the structure

我们仍然采取和之前一样的的MSE估计，此时，
$$L=\frac{1}{|P|}\sum_{x\in P}\log(\sum_z p(x,z))$$

简单起见，我们考虑求和中的一项$\log(\sum_z p(x,z))$,
由于这个式子本身就不好计算，梯度更加难算，我们用1.3.1中的Importance Sampling来估计这个值，具体地，我们引入一个分布$q(z)$,然后我们可以估计这个值为
$$\log p(x)=\log \sum_z q(z)\cdot \frac{p(x,z)}{q(z)} \ge^{Jensen}\sum_z  q(z)\log \frac{p(x,z)}{q(z)}$$

我们称右侧的这个下界为ELBO(Evidence Lower Bound)。
从而，我们可以通过从$q$中sample来估计后者从而估计前者。同时，为了让前后尽可能接近，我们训练$q$使得两者误差最小。注意到右侧事实上是
$$ELBO=\sum_z q(z)\log \frac{p(z|x)p(x)}{q(z)}=\log p(x)-KL(q(z)||p(z|x))$$

所以，在主训练步骤中，我们想要让$\log p(x)$尽量小，从而达到更好的效果，在训练$q$的过程中，我们想让它和$p$尽量接近，从而让取样模拟真实情况，方差更小。这时候再看上面那个式子，我们惊奇地发现，事实上这两件事都是在让ELBO最大化！所以，我们的两个过程都可以用一个统一的损失函数ELBO来训练。（你可能会注意到，这里虽然一切都吻合地非常好，但是也都是intuition,没有特别的必然性，所以其中的步骤仍然有调整的空间，$\beta-VAE$就是修改我们的ELBO，在KL散度前面加系数，使之取得更加灵活的效果）

**所以，我们的损失函数可以定义为上面的ELBO!**

至此，我们已经做出了一个比较完整的分析，在进入训练步骤之前，我们还需要考虑一个小小的改动。前面为了偷懒，我们只考虑了一个$x$的情形，然而，我们不能对每个$x$训练一个单独的$q$,于是我们转而考虑训练一个神经网络$q(z|x)$. 于是，我们的总体目标变成了训练两个神经网络$q_{\phi}$和$p_{\theta}$,使得
$$ELBO=\frac{1}{|P|}\sum_{x\in P}\sum_z q(z|x)\log\frac{p(z)}{q(z|x)}$$
$$=\frac{1}{|P|}\sum_{x\in P}\sum_z q(z|x)\log\frac{p(x|z)p(z)}{q(z|x)}$$
$$=\mathbb{E}_{z\sim q(z|x)}[\log p(x|z)]-KL(q(z|x)||p(z))$$

经过上面的数学变形，我们发现，这个loss有了直观的意义：第一项称为reconstruction loss, $q$就像一个编码器，把图片$x$编码成特征$z$，而 $p$ 则想以最大概率恢复原来的图片，第二项称为KL散度，它想让$q$和$p$尽量接近，从而让$q$的取样和真实情况更接近。

总结：
我们通过importance sampling的下界，找到了一个关键的损失函数$ELBO=\sum_z q(z|x)\log \frac{p(z)}{q(z|x)}$,它有两种表示
$$ELBO=\log p(x)-KL(q(z|x)||p(z|x))$$
$$ELBO=\mathbb{E}_{z\sim q(z|x)}[\log p(x|z)]-KL(q(z|x)||p(z))$$

第一种表示说明它在生成模型的训练和proposal的训练中都起到了evaluation的作用，所以可以作为统一的损失函数，而第二种表示则给出了直观的意义，同时变为了方便采样计算的结构。

##### 2.1.3.3 choose of $q$ and $p$
为了计算方便，我们取
$$p_{\theta}(z)=N(0,I)$$
$$p_{\theta}(x|z)=N(f_{\theta}(z),I)$$
$$q_{\phi}(z|x)=N(\mu_{\phi}(x),diag(\exp(\sigma_{\phi}(x))))$$
其中$f_{\theta},\mu_{\phi},\sigma_{\phi}$都是神经网络。

##### 2.1.3.4 Training

回到$ELBO$的可采样形式
$$ELBO=\mathbb{E}_{z\sim q_{\phi}(z|x)}[\log p_{\theta}(x|z)]-KL(q_{\phi}(z|x)||p_{\theta}(z))$$

我们注意到第二项就是两个高斯分布的KL散度，可以直接计算，而第一项则是一个期望，我们可以通过采样来估计。
具体地，当计算第一项对$\theta$的梯度的时候，可以从$q_{\phi}(z|x)$中采样若干个$z$，计算梯度平均值。而计算对$\phi$的梯度的时候，由于“采样”无法求导，我们可以通过一个小技巧，即把$N(a,b)$看成$a+b\cdot N(0,1)$, 这样我们的采样只涉及从标准正态分布中采样，从而总函数可以对$\phi$求导。一般来说，由于数据的随机性已经足够强，我们只进行一次采样估计。

综上所述，总训练过程如下：
- 1.初始化
- 2.训练
    - 2.1 训练$\theta$
        - 2.1.1 从$q_{\phi}(z|x)$中采样$z$
        - 2.1.2 通过采样估计$\nabla_{\theta}ELBO$
        - 2.1.3 更新$\theta$
    - 2.2 训练$\phi$
        - 2.2.1 从N(0,I)中采样$\epsilon$,根据它生成$z$
        - 2.2.2 估计$\nabla_{\phi}ELBO$
        - 2.2.3 更新$\phi$
- 3.生成
    - 3.1 从$p$中采样$z$
    - 3.2 从$p_{\theta}(x|z)$中采样$x$

##### 2.1.3.5 Others

###### 2.1.3.5.1 impainting

为了图片补全，我们需要$x\to z$的过程尽量robust, 这样我们就可以用需要补全的图片生成$z$，然后用$p_{\theta}(x|z)$生成图片。此时，我们可以在训练过程中随机mask掉一些neurons，从而让模型学会robust的特征。

###### 2.1.3.5.2 $\beta$-VAE

就是在上面已经说明的，通过在KL散度前面加一个系数$\beta$，从而可以调整reconstruction loss和KL散度的权重，从而可以调整模型的表现。

###### 2.1.3.5.3 Conditioned VAE

当数据有标签的时候应该如何处理？
此时，$p$变为$p_{\theta}(x|y,z)$,
$q$变为$q_{\phi}(y,z|x)$

对有标签的数据，我们只需要在loss中加入一项cross-entropy loss between $q(y|x)$和 $y$即可

对无标签的数据，我们让KL penalty 变成$KL(q(z)||p(z))+KL(q(y)||p(y))$, 对于$p(y)$我们可以取一个uniform distribution. 而 reconstruction loss 则变成$\mathbb{E}_{z,y\sim q(z,y)}[\log p(x|z,y)]$ 但此时$y$会变得难以sample从而不好计算梯度，当种类比较少的时候可以通过每一类枚举来计算，而种类比较多的时候我们将要使用一些特别的办法来处理，将会在之后说明。

### 2.2 Generative Adversarial Network